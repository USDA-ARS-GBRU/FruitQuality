{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0d9dfad9-6780-4b9e-b3a2-0b836738adb1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# autopep8: off\n",
    "import shutil\n",
    "import os\n",
    "os.environ['SM_FRAMEWORK'] = 'tf.keras'\n",
    "# autopep8: on\n",
    "\n",
    "import yaml\n",
    "import cv2\n",
    "import keras\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import albumentations as A\n",
    "import segmentation_models as sm\n",
    "import wandb\n",
    "\n",
    "backend=tf.keras.backend\n",
    "layers=tf.keras.layers\n",
    "models=tf.keras.models\n",
    "keras_utils = tf.keras.utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "339b4979-9d38-4471-83a3-887b3b790c0d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from keras_applications import get_submodules_from_kwargs\n",
    "\n",
    "from segmentation_models.models._common_blocks import Conv2dBn\n",
    "from segmentation_models.models._utils import freeze_model\n",
    "from segmentation_models.backbones.backbones_factory import Backbones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "df109305-177d-4e7f-af6f-5d68153ff45c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU'),\n",
       " PhysicalDevice(name='/physical_device:GPU:1', device_type='GPU')]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4ef6a553-0ae2-491a-b85d-9f071e7d299b",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = './Images/'\n",
    "MASK_DIR = './Masks/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6f2049ab-9360-49be-9e1b-8a15754d3e36",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def visualize(rows=1, **images):\n",
    "    \"\"\"PLot images in one row.\"\"\"\n",
    "    n = len(images)\n",
    "    plt.figure(figsize=(16, 5))\n",
    "    for i, (name, image) in enumerate(images.items()):\n",
    "        plt.subplot(rows, n//rows, i + 1)\n",
    "        plt.xticks([])\n",
    "        plt.yticks([])\n",
    "        plt.title(' '.join(name.split('_')).title())\n",
    "        plt.imshow(image)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def denormalize(x):\n",
    "    \"\"\"Scale image to range 0..1 for correct plot\"\"\"\n",
    "    x_max = np.percentile(x, 98)\n",
    "    x_min = np.percentile(x, 2)\n",
    "    x = (x - x_min) / (x_max - x_min)\n",
    "    x = x.clip(0, 1)\n",
    "    return x\n",
    "\n",
    "\n",
    "def image_to_same_shape(image, height, width):\n",
    "    if len(image.shape) == 2:\n",
    "        old_image_height, old_image_width = image.shape\n",
    "    else:\n",
    "        old_image_height, old_image_width, channels = image.shape\n",
    "\n",
    "    # create new image of desired size and color (blue) for padding\n",
    "    new_image_width = width\n",
    "    new_image_height = height\n",
    "    color = (0)\n",
    "    if len(image.shape) == 2:\n",
    "        result = np.full((new_image_height, new_image_width),\n",
    "                         color, dtype=np.uint8)\n",
    "    else:\n",
    "        result = np.full((new_image_height, new_image_width,\n",
    "                         channels), color, dtype=np.uint8)\n",
    "\n",
    "    # compute center offset\n",
    "    x_center = (new_image_width - old_image_width) // 2\n",
    "    y_center = (new_image_height - old_image_height) // 2\n",
    "\n",
    "    # copy img image into center of result image\n",
    "    result[y_center:y_center+old_image_height,\n",
    "           x_center:x_center+old_image_width] = image\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "class Dataset:\n",
    "    \"\"\"Dataset. Read images, apply augmentation and preprocessing transformations.\n",
    "\n",
    "    Args:\n",
    "        images_dir (str): path to images folder\n",
    "        masks_dir (str): path to segmentation masks folder\n",
    "        class_values (list): values of classes to extract from segmentation mask\n",
    "        augmentation (albumentations.Compose): data transfromation pipeline \n",
    "            (e.g. flip, scale, etc.)\n",
    "        preprocessing (albumentations.Compose): data preprocessing \n",
    "            (e.g. noralization, shape manipulation, etc.)\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    CLASSES = ['unlabelled', 'seed', 'pulp', 'albedo', 'flavedo']\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            ids,\n",
    "            images_dir,\n",
    "            masks_dir,\n",
    "            classes=None,\n",
    "            augmentation=None,\n",
    "            preprocessing=None,\n",
    "    ):\n",
    "        self.ids = ids\n",
    "        # self.ids = os.listdir(images_dir)\n",
    "        self.images_fps = [os.path.join(images_dir, image_id)\n",
    "                           for image_id in self.ids]\n",
    "        self.masks_fps = [os.path.join(\n",
    "            masks_dir, image_id)+'.png' for image_id in self.ids]\n",
    "\n",
    "        # convert str names to class values on masks\n",
    "        self.class_values = [self.CLASSES.index(\n",
    "            cls.lower()) for cls in classes]\n",
    "\n",
    "        self.augmentation = augmentation\n",
    "        self.preprocessing = preprocessing\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "\n",
    "        # read data\n",
    "        image = cv2.imread(self.images_fps[i])\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        mask = cv2.imread(self.masks_fps[i], 0)\n",
    "\n",
    "        image = image_to_same_shape(image, 1024, 1024)\n",
    "        mask = image_to_same_shape(mask, 1024, 1024)\n",
    "\n",
    "        # extract certain classes from mask (e.g. cars)\n",
    "        masks = [(mask == v*(255//4)) for v in self.class_values]\n",
    "        mask = np.stack(masks, axis=-1).astype('float')\n",
    "\n",
    "        # add background if mask is not binary\n",
    "        if mask.shape[-1] != 1:\n",
    "            background = 1 - mask.sum(axis=-1, keepdims=True)\n",
    "            mask = np.concatenate((mask, background), axis=-1)\n",
    "\n",
    "        # apply augmentations\n",
    "        if self.augmentation:\n",
    "            sample = self.augmentation(image=image, mask=mask)\n",
    "            image, mask = sample['image'], sample['mask']\n",
    "\n",
    "        # apply preprocessing\n",
    "        if self.preprocessing:\n",
    "            sample = self.preprocessing(image=image, mask=mask)\n",
    "            image, mask = sample['image'], sample['mask']\n",
    "\n",
    "        return image, mask\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ids)\n",
    "\n",
    "\n",
    "class_labels = {0: \"Seed\",\n",
    "                1: \"Pulp\",\n",
    "                2: \"Albedo\",\n",
    "                3: \"Flavedo\",\n",
    "                4: \"Background\"\n",
    "                }\n",
    "\n",
    "\n",
    "class Dataloder(keras.utils.Sequence):\n",
    "    \"\"\"Load data from dataset and form batches\n",
    "\n",
    "    Args:\n",
    "        dataset: instance of Dataset class for image loading and preprocessing.\n",
    "        batch_size: Integet number of images in batch.\n",
    "        shuffle: Boolean, if `True` shuffle image indexes each epoch.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dataset, batch_size=1, shuffle=False):\n",
    "        self.dataset = dataset\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.indexes = np.arange(len(dataset))\n",
    "\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "\n",
    "        # collect batch data\n",
    "        start = i * self.batch_size\n",
    "        stop = (i + 1) * self.batch_size\n",
    "        data = []\n",
    "        for j in range(start, stop):\n",
    "            data.append(self.dataset[j])\n",
    "\n",
    "        # transpose list of lists\n",
    "        batch = [np.stack(samples, axis=0) for samples in zip(*data)]\n",
    "\n",
    "        return batch\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Denotes the number of batches per epoch\"\"\"\n",
    "        return len(self.indexes) // self.batch_size\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        \"\"\"Callback function to shuffle indexes each epoch\"\"\"\n",
    "        if self.shuffle:\n",
    "            self.indexes = np.random.permutation(self.indexes)\n",
    "\n",
    "\n",
    "def get_preprocessing(preprocessing_fn):\n",
    "    \"\"\"Construct preprocessing transform\n",
    "\n",
    "    Args:\n",
    "        preprocessing_fn (callbale): data normalization function \n",
    "            (can be specific for each pretrained neural network)\n",
    "    Return:\n",
    "        transform: albumentations.Compose\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    _transform = [\n",
    "        A.Lambda(image=preprocessing_fn),\n",
    "    ]\n",
    "    return A.Compose(_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "94e4675d-6719-44f4-8afe-ef4ae119a0f9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def filter_keras_submodules(kwargs):\n",
    "    \"\"\"Selects only arguments that define keras_application submodules. \"\"\"\n",
    "    submodule_keys = kwargs.keys() & {'backend', 'layers', 'models', 'utils'}\n",
    "    return {key: kwargs[key] for key in submodule_keys}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "64982e77-7ba4-4167-9332-edb648b127fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 1\n",
    "CLASSES = ['seed', 'pulp', 'albedo', 'flavedo']\n",
    "LR = 0.0001\n",
    "EPOCHS = 10\n",
    "n_classes = len(CLASSES) + 1\n",
    "activation = 'softmax'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ad7cab82-ce38-41a2-ba36-3a1daba16ff0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = sm.Linknet(\"efficientnetb0\", classes=5, activation=\"softmax\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "63e173a3-60d7-4edd-9de3-615bd218ef4c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.layers.core.activation.Activation at 0x2b9028a8a90>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_layer(\"top_activation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "63e806ac-556b-4536-8304-0601d0ccbbf7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_submodules():\n",
    "    return {\n",
    "        'backend': backend,\n",
    "        'models': models,\n",
    "        'layers': layers,\n",
    "        'utils': keras_utils,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "5bf0addc-996a-4c0a-bad4-7f6e28c5aa2f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from segmentation_models.models.linknet import Conv3x3BnReLU, Conv1x1BnReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f27165f7-561e-4b93-91e7-8d7bc2d63e22",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def DecoderUpsamplingX2Block(filters, stage, use_batchnorm, branch):\n",
    "    conv_block1_name = 'decoder_stage{}a_{}'.format(stage, branch)\n",
    "    conv_block2_name = 'decoder_stage{}b_{}'.format(stage, branch)\n",
    "    conv_block3_name = 'decoder_stage{}c_{}'.format(stage, branch)\n",
    "    up_name = 'decoder_stage{}_upsampling_{}'.format(stage, branch)\n",
    "    add_name = 'decoder_stage{}_add_{}'.format(stage, branch)\n",
    "\n",
    "    channels_axis = 3 if backend.image_data_format() == 'channels_last' else 1\n",
    "\n",
    "    def wrapper(input_tensor, skip=None):\n",
    "        input_filters = backend.int_shape(input_tensor)[channels_axis]\n",
    "        output_filters = backend.int_shape(skip)[channels_axis] if skip is not None else filters\n",
    "\n",
    "        x = Conv1x1BnReLU(input_filters // 4, use_batchnorm, name=conv_block1_name)(input_tensor)\n",
    "        x = layers.UpSampling2D((2, 2), name=up_name)(x)\n",
    "        x = Conv3x3BnReLU(input_filters // 4, use_batchnorm, name=conv_block2_name)(x)\n",
    "        x = Conv1x1BnReLU(output_filters, use_batchnorm, name=conv_block3_name)(x)\n",
    "\n",
    "        if skip is not None:\n",
    "            x = layers.Add(name=add_name)([x, skip])\n",
    "        return x\n",
    "\n",
    "    return wrapper\n",
    "\n",
    "\n",
    "def DecoderTransposeX2Block(filters, stage, use_batchnorm, branch):\n",
    "    conv_block1_name = 'decoder_stage{}a_{}'.format(stage, branch)\n",
    "    transpose_name = 'decoder_stage{}b_transpose_{}'.format(stage, branch)\n",
    "    bn_name = 'decoder_stage{}b_bn_{}'.format(stage, branch)\n",
    "    relu_name = 'decoder_stage{}b_relu_{}'.format(stage, branch)\n",
    "    conv_block3_name = 'decoder_stage{}c_{}'.format(stage, branch)\n",
    "    add_name = 'decoder_stage{}_add_{}'.format(stage, branch)\n",
    "\n",
    "    channels_axis = bn_axis = 3 if backend.image_data_format() == 'channels_last' else 1\n",
    "\n",
    "    def wrapper(input_tensor, skip=None):\n",
    "        input_filters = backend.int_shape(input_tensor)[channels_axis]\n",
    "        output_filters = backend.int_shape(skip)[channels_axis] if skip is not None else filters\n",
    "\n",
    "        x = Conv1x1BnReLU(input_filters // 4, use_batchnorm, name=conv_block1_name)(input_tensor)\n",
    "        x = layers.Conv2DTranspose(\n",
    "            filters=input_filters // 4,\n",
    "            kernel_size=(4, 4),\n",
    "            strides=(2, 2),\n",
    "            padding='same',\n",
    "            name=transpose_name,\n",
    "            use_bias=not use_batchnorm,\n",
    "        )(x)\n",
    "\n",
    "        if use_batchnorm:\n",
    "            x = layers.BatchNormalization(axis=bn_axis, name=bn_name)(x)\n",
    "\n",
    "        x = layers.Activation('relu', name=relu_name)(x)\n",
    "        x = Conv1x1BnReLU(output_filters, use_batchnorm, name=conv_block3_name)(x)\n",
    "\n",
    "        if skip is not None:\n",
    "            x = layers.Add(name=add_name)([x, skip])\n",
    "\n",
    "        return x\n",
    "\n",
    "    return wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "37993647-1e2a-481a-bb65-7abc9bb8e9bf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def build_linknet(\n",
    "        backbone,\n",
    "        decoder_block,\n",
    "        skip_connection_layers,\n",
    "        decoder_filters=(256, 128, 64, 32, 16),\n",
    "        n_upsample_blocks=5,\n",
    "        classes=1,\n",
    "        activation='sigmoid',\n",
    "        use_batchnorm=True,\n",
    "        branch=0\n",
    "):\n",
    "    input_ = backbone.input\n",
    "    x = backbone.output\n",
    "\n",
    "    # extract skip connections\n",
    "    skips = ([backbone.get_layer(name=i).output if isinstance(i, str)\n",
    "              else backbone.get_layer(index=i).output for i in skip_connection_layers])\n",
    "\n",
    "    # add center block if previous operation was maxpooling (for vgg models)\n",
    "    if isinstance(backbone.layers[-1], layers.MaxPooling2D):\n",
    "        x = Conv3x3BnReLU(512, use_batchnorm, name='center_block1')(x)\n",
    "        x = Conv3x3BnReLU(512, use_batchnorm, name='center_block2')(x)\n",
    "\n",
    "    # building decoder blocks\n",
    "    for i in range(n_upsample_blocks):\n",
    "\n",
    "        if i < len(skips):\n",
    "            skip = skips[i]\n",
    "        else:\n",
    "            skip = None\n",
    "\n",
    "        x = decoder_block(decoder_filters[i], stage=i, use_batchnorm=use_batchnorm, branch=branch)(x, skip)\n",
    "\n",
    "    # model head (define number of output classes)\n",
    "    x = layers.Conv2D(\n",
    "        filters=classes,\n",
    "        kernel_size=(3, 3),\n",
    "        padding='same',\n",
    "        use_bias=True,\n",
    "        kernel_initializer='glorot_uniform'\n",
    "    )(x)\n",
    "    x = layers.Activation(activation, name=f\"{activation}_{branch}\")(x)\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "f2edbbad-6b27-4663-ab0d-cec47d851e00",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def Linknet(\n",
    "        backbone_name='vgg16',\n",
    "        input_shape=(None, None, 3),\n",
    "        classes=1,\n",
    "        activation='sigmoid',\n",
    "        weights=None,\n",
    "        encoder_weights='imagenet',\n",
    "        encoder_freeze=False,\n",
    "        encoder_features='default',\n",
    "        decoder_block_type='upsampling',\n",
    "        decoder_filters=(None, None, None, None, 16),\n",
    "        decoder_use_batchnorm=True,\n",
    "        decoder_branches=2,\n",
    "        **kwargs\n",
    "):\n",
    "\n",
    "    global backend, layers, models, keras_utils\n",
    "    submodule_args = filter_keras_submodules(kwargs)\n",
    "    backend, layers, models, keras_utils = get_submodules_from_kwargs(submodule_args)\n",
    "\n",
    "    if decoder_block_type == 'upsampling':\n",
    "        decoder_block = DecoderUpsamplingX2Block\n",
    "    elif decoder_block_type == 'transpose':\n",
    "        decoder_block = DecoderTransposeX2Block\n",
    "    else:\n",
    "        raise ValueError('Decoder block type should be in (\"upsampling\", \"transpose\"). '\n",
    "                         'Got: {}'.format(decoder_block_type))\n",
    "\n",
    "    backbone = Backbones.get_backbone(\n",
    "        backbone_name,\n",
    "        input_shape=input_shape,\n",
    "        weights=encoder_weights,\n",
    "        include_top=False,\n",
    "        **kwargs,\n",
    "    )\n",
    "\n",
    "    if encoder_features == 'default':\n",
    "        encoder_features = Backbones.get_feature_layers(backbone_name, n=4)\n",
    "\n",
    "    task1_branch_output  = build_linknet(\n",
    "        backbone=backbone,\n",
    "        decoder_block=decoder_block,\n",
    "        skip_connection_layers=encoder_features,\n",
    "        decoder_filters=decoder_filters,\n",
    "        classes=classes,\n",
    "        activation=activation,\n",
    "        n_upsample_blocks=len(decoder_filters),\n",
    "        use_batchnorm=decoder_use_batchnorm,\n",
    "        branch=0\n",
    "    )\n",
    "    \n",
    "    task2_branch_output  = build_linknet(\n",
    "        backbone=backbone,\n",
    "        decoder_block=decoder_block,\n",
    "        skip_connection_layers=encoder_features,\n",
    "        decoder_filters=decoder_filters,\n",
    "        classes=classes,\n",
    "        activation=activation,\n",
    "        n_upsample_blocks=len(decoder_filters),\n",
    "        use_batchnorm=decoder_use_batchnorm,\n",
    "        branch=1\n",
    "    )\n",
    "\n",
    "    # lock encoder weights for fine-tuning\n",
    "    if encoder_freeze:\n",
    "        freeze_model(backbone, **kwargs)\n",
    "\n",
    "    # loading model weights\n",
    "    if weights is not None:\n",
    "        model.load_weights(weights)\n",
    "        \n",
    "    model = tf.keras.Model(inputs = backbone.input, outputs = [task1_branch_output, task2_branch_output])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "9b2cadd7-ff70-4024-a76e-db9707709364",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "branched_model = Linknet(\"efficientnetb0\", classes=5, activation=\"softmax\", backend=backend, layers=layers, models=models, utils=keras_utils)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "d61199ba-aef6-4a0c-b29f-cb6eb4d6096f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# define optomizer\n",
    "optim = keras.optimizers.Adam(1e-3)\n",
    "\n",
    "# Segmentation models losses can be combined together by '+' and scaled by integer or float factor\n",
    "# set class weights for dice_loss (car: 1.; pedestrian: 2.; background: 0.5;)\n",
    "dice_loss = sm.losses.DiceLoss()\n",
    "focal_loss = sm.losses.CategoricalFocalLoss()\n",
    "total_loss = dice_loss + (1 * focal_loss)\n",
    "\n",
    "metrics = [sm.metrics.IOUScore(threshold=0.5),\n",
    "           sm.metrics.FScore(threshold=0.5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "2a175f61-b23e-4be8-b1a8-5fdbb0c0d34d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.compile(optim, \"mae\", metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "cacd97e8-52a2-4137-b20d-005b90042c8f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "branched_model.compile(optimizer=optim,\n",
    "                       loss={'softmax_0': \"mae\", \n",
    "                             'softmax_1': \"mae\"},\n",
    "                       metrics=metrics,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "3b12d132-08a4-4623-8e22-3a12547fc376",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_ids = [image_id.replace(\".png\", \"\") for image_id in os.listdir(MASK_DIR)]\n",
    "SIZE = len(data_ids)\n",
    "TRAIN_SIZE = int(0.6 * SIZE)\n",
    "VAL_SIZE = int(0.2 * SIZE)\n",
    "\n",
    "# Dataset for train images\n",
    "# train_dataset = dataset[:TRAIN_SIZE]\n",
    "train_dataset = Dataset(\n",
    "    data_ids[:TRAIN_SIZE],\n",
    "    DATA_DIR,\n",
    "    MASK_DIR,\n",
    "    classes=CLASSES,\n",
    ")\n",
    "\n",
    "# Dataset for validation images\n",
    "# valid_dataset = dataset[TRAIN_SIZE:VAL_SIZE]\n",
    "valid_dataset = Dataset(\n",
    "    data_ids[TRAIN_SIZE:TRAIN_SIZE+VAL_SIZE],\n",
    "    DATA_DIR,\n",
    "    MASK_DIR,\n",
    "    classes=CLASSES,\n",
    ")\n",
    "\n",
    "# Dataset for test images\n",
    "# test_dataset = dataset[VAL_SIZE:]\n",
    "test_dataset = Dataset(\n",
    "    data_ids[TRAIN_SIZE+VAL_SIZE:],\n",
    "    DATA_DIR,\n",
    "    MASK_DIR,\n",
    "    classes=CLASSES,\n",
    ")\n",
    "\n",
    "dataset_image, mask = test_dataset[0]  # get some sample\n",
    "\n",
    "train_dataloader = Dataloder(\n",
    "    train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "valid_dataloader = Dataloder(valid_dataset, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "3648ef20-4632-41da-81c2-1ae6484f20f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_dir = f'./keras_checkpoints/branching'\n",
    "if not os.path.exists(checkpoint_dir):\n",
    "    os.makedirs(checkpoint_dir)\n",
    "checkpoint_path = os.path.join(checkpoint_dir, \"best_model_{epoch:02d}\")\n",
    "callbacks = [\n",
    "    keras.callbacks.ModelCheckpoint(\n",
    "        checkpoint_path, save_weights_only=True, save_best_only=True),\n",
    "    keras.callbacks.ReduceLROnPlateau(),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "24dbfc68-c491-4eb0-99f0-2f497bfcaff3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Couldn't find program: 'false'\n"
     ]
    }
   ],
   "source": [
    "%%script false\n",
    "\n",
    "# train model\n",
    "history = model.fit(\n",
    "    train_dataloader,\n",
    "    steps_per_epoch=len(train_dataloader),\n",
    "    epochs=EPOCHS,\n",
    "    callbacks=callbacks,\n",
    "    validation_data=valid_dataloader,\n",
    "    validation_steps=len(valid_dataloader),\n",
    ")\n",
    "\n",
    "test_dataloader = Dataloder(test_dataset, batch_size=1, shuffle=False)\n",
    "# load best weights\n",
    "latest = tf.train.latest_checkpoint(checkpoint_dir)\n",
    "scores = model.evaluate(test_dataloader)\n",
    "model.load_weights(latest)\n",
    "scores = model.evaluate(test_dataloader)\n",
    "\n",
    "print(\"Loss: {:.5}\".format(scores[0]))\n",
    "for metric, value in zip(metrics, scores[1:]):\n",
    "    print(\"mean {}: {:.5}\".format(metric.__name__, value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "55c122c9-a590-4fe2-9da0-cfc1f0573208",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Couldn't find program: 'false'\n"
     ]
    }
   ],
   "source": [
    "%%script false\n",
    "\n",
    "del train_dataloader\n",
    "del valid_dataloader\n",
    "del test_dataloader\n",
    "del train_dataset\n",
    "del valid_dataset\n",
    "del test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "5f0d8146-8005-4c71-817e-9af2c1f562a2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "a85a3c90-7f22-420c-86ec-5ecbf1fdf5f4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Dataset2:\n",
    "    \"\"\"Dataset. Read images, apply augmentation and preprocessing transformations.\n",
    "\n",
    "    Args:\n",
    "        images_dir (str): path to images folder\n",
    "        masks_dir (str): path to segmentation masks folder\n",
    "        class_values (list): values of classes to extract from segmentation mask\n",
    "        augmentation (albumentations.Compose): data transfromation pipeline \n",
    "            (e.g. flip, scale, etc.)\n",
    "        preprocessing (albumentations.Compose): data preprocessing \n",
    "            (e.g. noralization, shape manipulation, etc.)\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    CLASSES = ['unlabelled', 'seed', 'pulp', 'albedo', 'flavedo']\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            ids,\n",
    "            images_dir,\n",
    "            masks_dir,\n",
    "            classes=None,\n",
    "            augmentation=None,\n",
    "            preprocessing=None,\n",
    "    ):\n",
    "        self.ids = ids\n",
    "        # self.ids = os.listdir(images_dir)\n",
    "        self.images_fps = [os.path.join(images_dir, image_id)\n",
    "                           for image_id in self.ids]\n",
    "        self.masks_fps = [os.path.join(\n",
    "            masks_dir, image_id)+'.png' for image_id in self.ids]\n",
    "\n",
    "        # convert str names to class values on masks\n",
    "        self.class_values = [self.CLASSES.index(\n",
    "            cls.lower()) for cls in classes]\n",
    "\n",
    "        self.augmentation = augmentation\n",
    "        self.preprocessing = preprocessing\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "\n",
    "        # read data\n",
    "        image = cv2.imread(self.images_fps[i])\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        mask = cv2.imread(self.masks_fps[i], 0)\n",
    "\n",
    "        image = image_to_same_shape(image, 1024, 1024)\n",
    "        mask = image_to_same_shape(mask, 1024, 1024)\n",
    "\n",
    "        # extract certain classes from mask (e.g. cars)\n",
    "        masks = [(mask == v*(255//4)) for v in self.class_values]\n",
    "        mask = np.stack(masks, axis=-1).astype('float')\n",
    "\n",
    "        # add background if mask is not binary\n",
    "        if mask.shape[-1] != 1:\n",
    "            background = 1 - mask.sum(axis=-1, keepdims=True)\n",
    "            mask = np.concatenate((mask, background), axis=-1)\n",
    "\n",
    "        # apply augmentations\n",
    "        if self.augmentation:\n",
    "            sample = self.augmentation(image=image, mask=mask)\n",
    "            image, mask = sample['image'], sample['mask']\n",
    "\n",
    "        # apply preprocessing\n",
    "        if self.preprocessing:\n",
    "            sample = self.preprocessing(image=image, mask=mask)\n",
    "            image, mask = sample['image'], sample['mask']\n",
    "\n",
    "        return image, [mask, mask]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "3247e407-47c4-4849-8ed0-cffc68acff4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_ids = [image_id.replace(\".png\", \"\") for image_id in os.listdir(MASK_DIR)]\n",
    "SIZE = len(data_ids)\n",
    "TRAIN_SIZE = int(0.6 * SIZE)\n",
    "VAL_SIZE = int(0.2 * SIZE)\n",
    "\n",
    "# Dataset for train images\n",
    "# train_dataset = dataset[:TRAIN_SIZE]\n",
    "train_dataset2 = Dataset2(\n",
    "    data_ids[:TRAIN_SIZE],\n",
    "    DATA_DIR,\n",
    "    MASK_DIR,\n",
    "    classes=CLASSES,\n",
    ")\n",
    "\n",
    "# Dataset for validation images\n",
    "# valid_dataset = dataset[TRAIN_SIZE:VAL_SIZE]\n",
    "valid_dataset2 = Dataset2(\n",
    "    data_ids[TRAIN_SIZE:TRAIN_SIZE+VAL_SIZE],\n",
    "    DATA_DIR,\n",
    "    MASK_DIR,\n",
    "    classes=CLASSES,\n",
    ")\n",
    "\n",
    "# Dataset for test images\n",
    "# test_dataset = dataset[VAL_SIZE:]\n",
    "test_dataset2 = Dataset2(\n",
    "    data_ids[TRAIN_SIZE+VAL_SIZE:],\n",
    "    DATA_DIR,\n",
    "    MASK_DIR,\n",
    "    classes=CLASSES,\n",
    ")\n",
    "\n",
    "# dataset_image2, (mask1, mask2) = test_dataset2[0]  # get some sample\n",
    "\n",
    "train_dataloader2 = Dataloder(\n",
    "    train_dataset2, batch_size=BATCH_SIZE, shuffle=True)\n",
    "valid_dataloader2 = Dataloder(valid_dataset2, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "5ceea8ae-0fb0-4986-bed4-55dd6edef984",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 2s 2s/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(2, 1, 1024, 1024, 5)"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = branched_model.predict(dataset_image[tf.newaxis, ...])\n",
    "np.array(output).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "93ee9a81-4626-4abd-a598-475a514603fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "Graph execution error:\n\nDetected at node 'mean_absolute_error/remove_squeezable_dimensions/Squeeze' defined at (most recent call last):\n    File \"C:\\Users\\shaki\\miniconda3\\envs\\fruitQuality\\lib\\runpy.py\", line 194, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"C:\\Users\\shaki\\miniconda3\\envs\\fruitQuality\\lib\\runpy.py\", line 87, in _run_code\n      exec(code, run_globals)\n    File \"C:\\Users\\shaki\\miniconda3\\envs\\fruitQuality\\lib\\site-packages\\ipykernel_launcher.py\", line 17, in <module>\n      app.launch_new_instance()\n    File \"C:\\Users\\shaki\\miniconda3\\envs\\fruitQuality\\lib\\site-packages\\traitlets\\config\\application.py\", line 1043, in launch_instance\n      app.start()\n    File \"C:\\Users\\shaki\\miniconda3\\envs\\fruitQuality\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 712, in start\n      self.io_loop.start()\n    File \"C:\\Users\\shaki\\miniconda3\\envs\\fruitQuality\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 215, in start\n      self.asyncio_loop.run_forever()\n    File \"C:\\Users\\shaki\\miniconda3\\envs\\fruitQuality\\lib\\asyncio\\base_events.py\", line 570, in run_forever\n      self._run_once()\n    File \"C:\\Users\\shaki\\miniconda3\\envs\\fruitQuality\\lib\\asyncio\\base_events.py\", line 1859, in _run_once\n      handle._run()\n    File \"C:\\Users\\shaki\\miniconda3\\envs\\fruitQuality\\lib\\asyncio\\events.py\", line 81, in _run\n      self._context.run(self._callback, *self._args)\n    File \"C:\\Users\\shaki\\miniconda3\\envs\\fruitQuality\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 510, in dispatch_queue\n      await self.process_one()\n    File \"C:\\Users\\shaki\\miniconda3\\envs\\fruitQuality\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 499, in process_one\n      await dispatch(*args)\n    File \"C:\\Users\\shaki\\miniconda3\\envs\\fruitQuality\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 406, in dispatch_shell\n      await result\n    File \"C:\\Users\\shaki\\miniconda3\\envs\\fruitQuality\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 730, in execute_request\n      reply_content = await reply_content\n    File \"C:\\Users\\shaki\\miniconda3\\envs\\fruitQuality\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 383, in do_execute\n      res = shell.run_cell(\n    File \"C:\\Users\\shaki\\miniconda3\\envs\\fruitQuality\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 528, in run_cell\n      return super().run_cell(*args, **kwargs)\n    File \"C:\\Users\\shaki\\miniconda3\\envs\\fruitQuality\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2961, in run_cell\n      result = self._run_cell(\n    File \"C:\\Users\\shaki\\miniconda3\\envs\\fruitQuality\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3016, in _run_cell\n      result = runner(coro)\n    File \"C:\\Users\\shaki\\miniconda3\\envs\\fruitQuality\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 129, in _pseudo_sync_runner\n      coro.send(None)\n    File \"C:\\Users\\shaki\\miniconda3\\envs\\fruitQuality\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3221, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"C:\\Users\\shaki\\miniconda3\\envs\\fruitQuality\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3400, in run_ast_nodes\n      if await self.run_code(code, result, async_=asy):\n    File \"C:\\Users\\shaki\\miniconda3\\envs\\fruitQuality\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3460, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"C:\\Users\\shaki\\AppData\\Local\\Temp\\ipykernel_22804\\3465977345.py\", line 2, in <module>\n      history = branched_model.fit(\n    File \"C:\\Users\\shaki\\miniconda3\\envs\\fruitQuality\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\shaki\\miniconda3\\envs\\fruitQuality\\lib\\site-packages\\keras\\engine\\training.py\", line 1564, in fit\n      tmp_logs = self.train_function(iterator)\n    File \"C:\\Users\\shaki\\miniconda3\\envs\\fruitQuality\\lib\\site-packages\\keras\\engine\\training.py\", line 1160, in train_function\n      return step_function(self, iterator)\n    File \"C:\\Users\\shaki\\miniconda3\\envs\\fruitQuality\\lib\\site-packages\\keras\\engine\\training.py\", line 1146, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Users\\shaki\\miniconda3\\envs\\fruitQuality\\lib\\site-packages\\keras\\engine\\training.py\", line 1135, in run_step\n      outputs = model.train_step(data)\n    File \"C:\\Users\\shaki\\miniconda3\\envs\\fruitQuality\\lib\\site-packages\\keras\\engine\\training.py\", line 994, in train_step\n      loss = self.compute_loss(x, y, y_pred, sample_weight)\n    File \"C:\\Users\\shaki\\miniconda3\\envs\\fruitQuality\\lib\\site-packages\\keras\\engine\\training.py\", line 1052, in compute_loss\n      return self.compiled_loss(\n    File \"C:\\Users\\shaki\\miniconda3\\envs\\fruitQuality\\lib\\site-packages\\keras\\engine\\compile_utils.py\", line 265, in __call__\n      loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"C:\\Users\\shaki\\miniconda3\\envs\\fruitQuality\\lib\\site-packages\\keras\\losses.py\", line 152, in __call__\n      losses = call_fn(y_true, y_pred)\n    File \"C:\\Users\\shaki\\miniconda3\\envs\\fruitQuality\\lib\\site-packages\\keras\\losses.py\", line 265, in call\n      y_pred, y_true = losses_utils.squeeze_or_expand_dimensions(\n    File \"C:\\Users\\shaki\\miniconda3\\envs\\fruitQuality\\lib\\site-packages\\keras\\utils\\losses_utils.py\", line 200, in squeeze_or_expand_dimensions\n      y_true, y_pred = remove_squeezable_dimensions(y_true, y_pred)\n    File \"C:\\Users\\shaki\\miniconda3\\envs\\fruitQuality\\lib\\site-packages\\keras\\utils\\losses_utils.py\", line 139, in remove_squeezable_dimensions\n      labels = tf.squeeze(labels, [-1])\nNode: 'mean_absolute_error/remove_squeezable_dimensions/Squeeze'\nCan not squeeze dim[4], expected a dimension of 1, got 5\n\t [[{{node mean_absolute_error/remove_squeezable_dimensions/Squeeze}}]] [Op:__inference_train_function_135794]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[101], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# train model\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mbranched_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_dataloader2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43msteps_per_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataloader2\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mEPOCHS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalid_dataloader2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mvalid_dataloader2\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m test_dataloader2 \u001b[38;5;241m=\u001b[39m Dataloder(test_dataset2, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# load best weights\u001b[39;00m\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\fruitQuality\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\fruitQuality\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 54\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_Py_Execute(ctx\u001b[38;5;241m.\u001b[39m_handle, device_name, op_name,\n\u001b[0;32m     55\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     57\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: Graph execution error:\n\nDetected at node 'mean_absolute_error/remove_squeezable_dimensions/Squeeze' defined at (most recent call last):\n    File \"C:\\Users\\shaki\\miniconda3\\envs\\fruitQuality\\lib\\runpy.py\", line 194, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"C:\\Users\\shaki\\miniconda3\\envs\\fruitQuality\\lib\\runpy.py\", line 87, in _run_code\n      exec(code, run_globals)\n    File \"C:\\Users\\shaki\\miniconda3\\envs\\fruitQuality\\lib\\site-packages\\ipykernel_launcher.py\", line 17, in <module>\n      app.launch_new_instance()\n    File \"C:\\Users\\shaki\\miniconda3\\envs\\fruitQuality\\lib\\site-packages\\traitlets\\config\\application.py\", line 1043, in launch_instance\n      app.start()\n    File \"C:\\Users\\shaki\\miniconda3\\envs\\fruitQuality\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 712, in start\n      self.io_loop.start()\n    File \"C:\\Users\\shaki\\miniconda3\\envs\\fruitQuality\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 215, in start\n      self.asyncio_loop.run_forever()\n    File \"C:\\Users\\shaki\\miniconda3\\envs\\fruitQuality\\lib\\asyncio\\base_events.py\", line 570, in run_forever\n      self._run_once()\n    File \"C:\\Users\\shaki\\miniconda3\\envs\\fruitQuality\\lib\\asyncio\\base_events.py\", line 1859, in _run_once\n      handle._run()\n    File \"C:\\Users\\shaki\\miniconda3\\envs\\fruitQuality\\lib\\asyncio\\events.py\", line 81, in _run\n      self._context.run(self._callback, *self._args)\n    File \"C:\\Users\\shaki\\miniconda3\\envs\\fruitQuality\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 510, in dispatch_queue\n      await self.process_one()\n    File \"C:\\Users\\shaki\\miniconda3\\envs\\fruitQuality\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 499, in process_one\n      await dispatch(*args)\n    File \"C:\\Users\\shaki\\miniconda3\\envs\\fruitQuality\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 406, in dispatch_shell\n      await result\n    File \"C:\\Users\\shaki\\miniconda3\\envs\\fruitQuality\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 730, in execute_request\n      reply_content = await reply_content\n    File \"C:\\Users\\shaki\\miniconda3\\envs\\fruitQuality\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 383, in do_execute\n      res = shell.run_cell(\n    File \"C:\\Users\\shaki\\miniconda3\\envs\\fruitQuality\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 528, in run_cell\n      return super().run_cell(*args, **kwargs)\n    File \"C:\\Users\\shaki\\miniconda3\\envs\\fruitQuality\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2961, in run_cell\n      result = self._run_cell(\n    File \"C:\\Users\\shaki\\miniconda3\\envs\\fruitQuality\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3016, in _run_cell\n      result = runner(coro)\n    File \"C:\\Users\\shaki\\miniconda3\\envs\\fruitQuality\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 129, in _pseudo_sync_runner\n      coro.send(None)\n    File \"C:\\Users\\shaki\\miniconda3\\envs\\fruitQuality\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3221, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"C:\\Users\\shaki\\miniconda3\\envs\\fruitQuality\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3400, in run_ast_nodes\n      if await self.run_code(code, result, async_=asy):\n    File \"C:\\Users\\shaki\\miniconda3\\envs\\fruitQuality\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3460, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"C:\\Users\\shaki\\AppData\\Local\\Temp\\ipykernel_22804\\3465977345.py\", line 2, in <module>\n      history = branched_model.fit(\n    File \"C:\\Users\\shaki\\miniconda3\\envs\\fruitQuality\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\shaki\\miniconda3\\envs\\fruitQuality\\lib\\site-packages\\keras\\engine\\training.py\", line 1564, in fit\n      tmp_logs = self.train_function(iterator)\n    File \"C:\\Users\\shaki\\miniconda3\\envs\\fruitQuality\\lib\\site-packages\\keras\\engine\\training.py\", line 1160, in train_function\n      return step_function(self, iterator)\n    File \"C:\\Users\\shaki\\miniconda3\\envs\\fruitQuality\\lib\\site-packages\\keras\\engine\\training.py\", line 1146, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Users\\shaki\\miniconda3\\envs\\fruitQuality\\lib\\site-packages\\keras\\engine\\training.py\", line 1135, in run_step\n      outputs = model.train_step(data)\n    File \"C:\\Users\\shaki\\miniconda3\\envs\\fruitQuality\\lib\\site-packages\\keras\\engine\\training.py\", line 994, in train_step\n      loss = self.compute_loss(x, y, y_pred, sample_weight)\n    File \"C:\\Users\\shaki\\miniconda3\\envs\\fruitQuality\\lib\\site-packages\\keras\\engine\\training.py\", line 1052, in compute_loss\n      return self.compiled_loss(\n    File \"C:\\Users\\shaki\\miniconda3\\envs\\fruitQuality\\lib\\site-packages\\keras\\engine\\compile_utils.py\", line 265, in __call__\n      loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"C:\\Users\\shaki\\miniconda3\\envs\\fruitQuality\\lib\\site-packages\\keras\\losses.py\", line 152, in __call__\n      losses = call_fn(y_true, y_pred)\n    File \"C:\\Users\\shaki\\miniconda3\\envs\\fruitQuality\\lib\\site-packages\\keras\\losses.py\", line 265, in call\n      y_pred, y_true = losses_utils.squeeze_or_expand_dimensions(\n    File \"C:\\Users\\shaki\\miniconda3\\envs\\fruitQuality\\lib\\site-packages\\keras\\utils\\losses_utils.py\", line 200, in squeeze_or_expand_dimensions\n      y_true, y_pred = remove_squeezable_dimensions(y_true, y_pred)\n    File \"C:\\Users\\shaki\\miniconda3\\envs\\fruitQuality\\lib\\site-packages\\keras\\utils\\losses_utils.py\", line 139, in remove_squeezable_dimensions\n      labels = tf.squeeze(labels, [-1])\nNode: 'mean_absolute_error/remove_squeezable_dimensions/Squeeze'\nCan not squeeze dim[4], expected a dimension of 1, got 5\n\t [[{{node mean_absolute_error/remove_squeezable_dimensions/Squeeze}}]] [Op:__inference_train_function_135794]"
     ]
    }
   ],
   "source": [
    "# train model\n",
    "history = branched_model.fit(\n",
    "    train_dataloader2,\n",
    "    steps_per_epoch=len(train_dataloader2),\n",
    "    epochs=EPOCHS,\n",
    "    callbacks=callbacks,\n",
    "    validation_data=valid_dataloader2,\n",
    "    validation_steps=len(valid_dataloader2),\n",
    ")\n",
    "\n",
    "test_dataloader2 = Dataloder(test_dataset2, batch_size=1, shuffle=False)\n",
    "# load best weights\n",
    "latest = tf.train.latest_checkpoint(checkpoint_dir)\n",
    "branched_model.load_weights(latest)\n",
    "scores = branched_model.evaluate(test_dataloader2)\n",
    "\n",
    "print(\"Loss: {:.5}\".format(scores[0]))\n",
    "for metric, value in zip(metrics, scores[1:]):\n",
    "    print(\"mean {}: {:.5}\".format(metric.__name__, value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "4f372ae4-c218-404b-acf2-bf2ce37e0758",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.8.16\n"
     ]
    }
   ],
   "source": [
    "!python --version"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
